#!/usr/bin/env python3
"""
æ‰¹é‡åŒæ­¥æ–‡ç« è„šæœ¬
é‡æ–°çˆ¬å–æ‰€æœ‰æ²¡æœ‰å†…å®¹çš„æ–‡ç« 
"""

import asyncio
import sys
import random
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import aiohttp
import aiofiles
from src.core.database import get_async_session
from src.repository.article_repository import ArticleRepository
from src.repository.source_repository import SourceRepository
from src.services.universal_scraper import UniversalScraper
from src.core.models import ParserConfig, FetchStatus
from urllib.parse import unquote, parse_qs, urlparse


# è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼šéšæœº1-3ç§’ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
MIN_DELAY = 1.0
MAX_DELAY = 3.0


async def sync_articles():
    """æ‰¹é‡åŒæ­¥æ‰€æœ‰æ–‡ç« """
    print("å¼€å§‹æ‰¹é‡åŒæ­¥æ–‡ç« ...")

    async with get_async_session() as db:
        article_repo = ArticleRepository(db)
        source_repo = SourceRepository(db)

        # æŸ¥æ‰¾æ‰€æœ‰éœ€è¦åŒæ­¥çš„æ–‡ç« 
        sql = """
            SELECT id, url, source_id, title
            FROM articles
            WHERE content IS NULL OR length(content) < 100
            ORDER BY id ASC
        """
        articles = await article_repo.fetch_all(sql, {})

        total = len(articles)
        print(f"æ‰¾åˆ° {total} æ¡éœ€è¦åŒæ­¥çš„æ–‡ç« ")

        if total == 0:
            print("æ²¡æœ‰éœ€è¦åŒæ­¥çš„æ–‡ç« ")
            return

        success_count = 0
        failed_count = 0

        for idx, article in enumerate(articles, 1):
            article_id = article["id"]
            url = article["url"]

            print(f"\n[{idx}/{total}] å¤„ç†æ–‡ç«  {article_id}: {article['title'][:50]}")
            print(f"  URL: {url}")

            try:
                # è·å–æºé…ç½®
                source = await source_repo.fetch_by_id(article["source_id"])
                if not source:
                    print(f"  âŒ æº {article['source_id']} ä¸å­˜åœ¨")
                    failed_count += 1
                    continue

                # å¤„ç† parser_config
                parser_config = source.get("parser_config")
                if isinstance(parser_config, str):
                    parser_config = ParserConfig.model_validate_json(parser_config)
                elif isinstance(parser_config, dict):
                    parser_config = ParserConfig(**parser_config)

                # è§£æ DDG URL
                url_to_fetch = url
                if 'duckduckgo.com/l/' in url_to_fetch and 'uddg=' in url_to_fetch:
                    try:
                        parsed = urlparse(url_to_fetch)
                        params = parse_qs(parsed.query)
                        if 'uddg' in params:
                            encoded_url = params['uddg'][0]
                            url_to_fetch = unquote(encoded_url)
                            print(f"  ğŸ”“ è§£æ DDG URL -> {url_to_fetch}")
                    except Exception as e:
                        print(f"  âš ï¸  è§£æ DDG URL å¤±è´¥: {e}")

                # çˆ¬å–æ–‡ç« 
                async with UniversalScraper() as scraper:
                    scraped = await scraper.scrape(
                        url=url_to_fetch,
                        parser_config=parser_config or ParserConfig(
                            title_selector="h1",
                            content_selector="article, main",
                        ),
                        source_id=article["source_id"],
                    )

                    # æ£€æŸ¥ç»“æœ
                    if scraped.error:
                        print(f"  âŒ çˆ¬å–å¤±è´¥: {scraped.error}")
                        failed_count += 1

                        # æ›´æ–°ä¸ºå¤±è´¥çŠ¶æ€
                        await article_repo.update(article_id, {
                            "fetch_status": FetchStatus.FAILED.value,
                            "error_msg": scraped.error,
                        })
                    else:
                        # ä¸¥æ ¼çš„å†…å®¹éªŒè¯
                        content = scraped.content
                        error_msg = None
                        is_valid = True

                        # 1. æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–å¤ªçŸ­
                        if not content or len(content) < 50:
                            is_valid = False
                            error_msg = f"å†…å®¹å¤ªçŸ­ ({len(content) if content else 0} å­—ç¬¦ < 50)"

                        # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ•ˆå†…å®¹æ ‡è®°
                        elif any(keyword in content.lower() for keyword in [
                            "javascript", "enable javascript", "è¯·å¯ç”¨ javascript",
                            "è¯·å¼€å¯javascript", "éœ€è¦javascript", "enable cookies"
                        ]):
                            is_valid = False
                            error_msg = "å†…å®¹åŒ…å«æ— æ•ˆæ ‡è®° (javascript/cookies)"

                        # 3. æ£€æŸ¥æ˜¯å¦æå–åˆ°æ—¶é—´
                        elif not scraped.publish_time:
                            is_valid = False
                            error_msg = "æœªèƒ½æå–å‘å¸ƒæ—¶é—´"

                        # æ›´æ–°æ–‡ç« å†…å®¹
                        update_data = {
                            "title": scraped.title or article["title"],
                            "content": content if is_valid else None,
                            "publish_time": scraped.publish_time,
                            "author": scraped.author,
                            "fetch_status": FetchStatus.SUCCESS.value if is_valid else FetchStatus.FAILED.value,
                            "error_msg": None if is_valid else error_msg,
                        }

                        await article_repo.update(article_id, update_data)

                        if is_valid:
                            success_count += 1
                            print(f"  âœ… æˆåŠŸï¼")
                            print(f"     å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                            print(f"     å‘å¸ƒæ—¶é—´: {scraped.publish_time}")
                            print(f"     æ ‡é¢˜: {scraped.title}")

                            # ç«‹å³éªŒè¯
                            verify = await article_repo.fetch_by_id(article_id)
                            if verify and verify.get("content"):
                                print(f"  âœ“ éªŒè¯æˆåŠŸï¼Œæ•°æ®åº“å·²æ›´æ–°")
                            else:
                                print(f"  âš ï¸  è­¦å‘Šï¼šæ•°æ®åº“æ›´æ–°å¯èƒ½å¤±è´¥")
                        else:
                            failed_count += 1
                            print(f"  âŒ å¤±è´¥: {error_msg}")
                            if scraped.publish_time:
                                print(f"     (æ—¶é—´: {scraped.publish_time})")

            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                failed_count += 1

            # è¯·æ±‚é—´éš”ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼Œé™ä½è¢«å°ç¦é£é™©
            if idx < total:
                delay = random.uniform(MIN_DELAY, MAX_DELAY)
                print(f"  â³ ç­‰å¾… {delay:.1f} ç§’...")
                await asyncio.sleep(delay)

        print(f"\n{'='*60}")
        print(f"åŒæ­¥å®Œæˆï¼")
        print(f"æ€»è®¡: {total}")
        print(f"æˆåŠŸ: {success_count}")
        print(f"å¤±è´¥: {failed_count}")
        print(f"{'='*60}")


if __name__ == "__main__":
    asyncio.run(sync_articles())
